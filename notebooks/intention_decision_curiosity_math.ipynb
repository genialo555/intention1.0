{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a69781c",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# declare a list tasks whose products you want to use as inputs\n",
    "upstream = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667da02e",
   "metadata": {},
   "source": [
    "# Intention, Décision & Curiosité – Formules Mathématiques & Physiques\n",
    "\n",
    "Ce carnet Jupyter rassemble **les principales équations** issues de la psychologie quantitative, de l’économie comportementale, de la neuroscience computationnelle **et des analogies physiques** qui sous‑tendent les concepts d’intention, de prise de décision et de curiosité intrinsèque.  \n",
    "Chaque section contient :\n",
    "\n",
    "* un rappel théorique avec formules en \\\\( \\LaTeX \\\\),\n",
    "* des cellules Python illustratives (simulations, tracés),\n",
    "* des **exercices** (+ solutions en fin de section).\n",
    "\n",
    "Exécute les cellules pas à pas, modifie les paramètres et… reste curieux !\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2635b21e",
   "metadata": {},
   "source": [
    "## 1. Modéliser l’intention\n",
    "\n",
    "### 1.1 Théorie du comportement planifié (Ajzen)\n",
    "\n",
    "\\\\[\n",
    "\\text{Intention} = w_1 \\, \\text{Attitude} \\;+\\; w_2 \\, \\text{Norme subjective} \\;+\\; w_3 \\, \\text{Contrôle perçu}\n",
    "\\\\]\n",
    "\n",
    "où  \n",
    "* **Attitude** : évaluation personnelle du comportement,  \n",
    "* **Norme subjective** : pression sociale perçue,  \n",
    "* **Contrôle comportemental perçu** : facilité/difficulté anticipée,  \n",
    "* \\\\(w_i\\\\) : poids (estimés par régression logistique).\n",
    "\n",
    "### 1.2 Implémentation : prédire l’intention de pratiquer une activité physique\n",
    "\n",
    "Dans l’exemple ci‑dessous, on :\n",
    "\n",
    "1. génère un échantillon synthétique (N = 250),\n",
    "2. ajuste une **régression logistique**,\n",
    "3. visualise la probabilité d’intention en fonction des trois prédicteurs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afafd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Génération de données synthétiques\n",
    "np.random.seed(42)\n",
    "N = 250\n",
    "att = np.random.normal(0, 1, N)\n",
    "norm = np.random.normal(0, 1, N)\n",
    "ctrl = np.random.normal(0, 1, N)\n",
    "# vrais poids\n",
    "w = np.array([1.2, 0.8, 1.0])\n",
    "z = 0.5 + w[0]*att + w[1]*norm + w[2]*ctrl\n",
    "prob = 1/(1+np.exp(-z))\n",
    "intent = (np.random.rand(N) < prob).astype(int)\n",
    "\n",
    "df = pd.DataFrame({'Attitude': att, 'Norme': norm, 'Contrôle': ctrl, 'Intention': intent})\n",
    "\n",
    "# Modèle\n",
    "X = df[['Attitude', 'Norme', 'Contrôle']]\n",
    "y = df['Intention']\n",
    "model = LogisticRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "print('Poids estimés :', np.round(model.coef_[0], 2))\n",
    "print('Intercept :   ', np.round(model.intercept_, 2))\n",
    "\n",
    "# Visualisation 2‑D pour Attitude vs Probabilité (les autres fixés à 0)\n",
    "att_grid = np.linspace(-3, 3, 200)\n",
    "X_plot = np.c_[att_grid, np.zeros_like(att_grid), np.zeros_like(att_grid)]\n",
    "p_plot = model.predict_proba(X_plot)[:,1]\n",
    "\n",
    "plt.plot(att_grid, p_plot)\n",
    "plt.xlabel('Attitude (écart‑type)')\n",
    "plt.ylabel('P(Intention = 1)')\n",
    "plt.title(\"Probabilité d'intention selon l'attitude\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb9bfe6",
   "metadata": {},
   "source": [
    "**Exercice 1.1 :**  \n",
    "*Change les poids \\\\(w_i\\\\) (ou le nombre d’observations) et observe comment la courbe logistique évolue.*  \n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e77ad47",
   "metadata": {},
   "source": [
    "## 2. Théorie de la décision\n",
    "\n",
    "### 2.1 Utilité espérée\n",
    "\n",
    "\\\\[\n",
    "EU(a) = \\sum_{i=1}^n p_i \\, u(x_i)\n",
    "\\\\]\n",
    "\n",
    "où \\\\(p_i\\\\) est la probabilité de l’issue \\\\(x_i\\\\), et \\\\(u\\\\) la fonction d’utilité.\n",
    "\n",
    "### 2.2 Théorie des perspectives (Kahneman & Tversky)\n",
    "\n",
    "Valeur :\n",
    "\n",
    "\\\\[\n",
    "v(x)=\n",
    "\\\\begin{cases}\n",
    "  x^{\\\\alpha}, & \\\\text{si } x \\\\ge 0,\\\\\\\\\n",
    "  -\\\\lambda\\\\,(-x)^{\\\\beta}, & \\\\text{si } x < 0,\n",
    "\\\\end{cases}\n",
    "\\\\]\n",
    "\n",
    "avec \\\\(\\\\lambda>1\\\\) (aversion aux pertes), \\\\(0<\\\\alpha,\\\\beta<1\\\\).\n",
    "\n",
    "Pondération des probabilités :\n",
    "\n",
    "\\\\[\n",
    "\\\\pi(p)=\\\\frac{p^{\\\\gamma}}{\\\\bigl(p^{\\\\gamma}+(1-p)^{\\\\gamma}\\\\bigr)^{1/\\\\gamma}}\n",
    "\\\\]\n",
    "\n",
    "### 2.3 Règle softmax\n",
    "\n",
    "\\\\[\n",
    "P(a)=\\\\frac{\\\\exp(\\\\beta Q(a))}{\\\\sum_b \\\\exp(\\\\beta Q(b))}\n",
    "\\\\]\n",
    "\n",
    "avec \\\\(\\\\beta\\\\) : *“inverse temperature”* (contrôle l’exploitation/exploration).\n",
    "\n",
    "### 2.4 Simulation du modèle drift‑diffusion (DDM)\n",
    "\n",
    "Le DDM modélise le temps et l’exactitude d’une décision binaire :\n",
    "\n",
    "\\\\[\n",
    "dx_t = k \\\\, dt + \\\\sigma \\\\, dW_t,\n",
    "\\\\quad\n",
    "\\\\text{décision quand } |x_t| \\\\ge A\n",
    "\\\\]\n",
    "\n",
    "où  \n",
    "* \\\\(k\\\\) : dérive (evidence),  \n",
    "* \\\\(\\\\sigma\\\\) : bruit,  \n",
    "* \\\\(A\\\\) : seuil.\n",
    "\n",
    "Le code suivant simule 500 trials et trace la distribution des temps de décision. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da300d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def simulate_ddm(n_trials=500, k=0.3, sigma=1.0, A=1.0, dt=0.01):\n",
    "    rts = []\n",
    "    choices = []\n",
    "    for _ in range(n_trials):\n",
    "        x = 0\n",
    "        t = 0\n",
    "        while abs(x) < A:\n",
    "            dx = k*dt + sigma*np.sqrt(dt)*np.random.randn()\n",
    "            x += dx\n",
    "            t += dt\n",
    "        rts.append(t)\n",
    "        choices.append(int(x > 0))\n",
    "    return np.array(rts), np.array(choices)\n",
    "\n",
    "rts, choices = simulate_ddm()\n",
    "print('Exactitude :', choices.mean())\n",
    "plt.hist(rts, bins=30)\n",
    "plt.xlabel('Temps de décision (s)')\n",
    "plt.ylabel('Fréquence')\n",
    "plt.title('Distribution des RT (DDM simulé)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e33f22c",
   "metadata": {},
   "source": [
    "**Exercice 2.1 :**  \n",
    "*Varie \\\\(k\\\\), \\\\(\\\\sigma\\\\) ou \\\\(A\\\\) et mesure l’effet sur la vitesse et la précision.*  \n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc29ed2",
   "metadata": {},
   "source": [
    "## 3. Curiosité & théorie de l’information\n",
    "\n",
    "### 3.1 Entropie de Shannon  \n",
    "\n",
    "\\\\[\n",
    "H(X) = -\\\\sum_{x} p(x)\\\\,\\\\log_2 p(x)\n",
    "\\\\]\n",
    "\n",
    "### 3.2 Divergence de Kullback–Leibler  \n",
    "\n",
    "\\\\[\n",
    "D_{\\\\text{KL}}\\\\bigl(P \\\\parallel Q\\\\bigr)=\\\\sum_x P(x)\\\\,\\\\log\\\\frac{P(x)}{Q(x)}\n",
    "\\\\]\n",
    "\n",
    "### 3.3 Gain d’information (mutual information)\n",
    "\n",
    "\\\\[\n",
    "I(X;Y)=H(X)-H(X\\\\mid Y)\n",
    "\\\\]\n",
    "\n",
    "### 3.4 Free‑energy (active inference, Friston)\n",
    "\n",
    "\\\\[\n",
    "\\\\mathcal F = \\\\mathbb E_q\\\\bigl[\\\\ln q(s) - \\\\ln p(s,o)\\\\bigr]\n",
    "\\\\]\n",
    "\n",
    "Minimiser \\\\(\\\\mathcal F\\\\) ≈ maximiser l’**exactitude** tout en minimisant la **complexité**.\n",
    "\n",
    "### 3.5 Bonus de curiosité (ICM)\n",
    "\n",
    "\\\\[\n",
    "R_t^{\\\\text{int}} = \\\\tfrac{1}{2}\\\\Bigl\\\\| \\\\hat\\\\phi(s_{t+1}) - \\\\phi(s_{t+1}) \\\\Bigr\\\\|^2\n",
    "\\\\]\n",
    "\n",
    "avec \\\\(\\\\phi\\\\) : encodeur d’état, \\\\(\\\\hat\\\\phi\\\\) : prédiction du modèle dynamique interne.\n",
    "\n",
    "#### Exemple : entropie d’une pièce biaisée\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd900ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "p = 0.3  # probabilité de pile\n",
    "entropy = - (p*math.log2(p) + (1-p)*math.log2(1-p))\n",
    "print(f\"Entropie d'une pièce biaisée (p={p}) : {entropy:.3f} bits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179f2193",
   "metadata": {},
   "source": [
    "**Exercice 3.1 :**  \n",
    "*Écris une fonction qui prend une liste de probabilités \\\\(p_i\\\\) et renvoie l’entropie correspondante. Teste‑la avec une loi uniforme (maximum d’entropie) puis avec une loi très concentrée.*  \n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a65bc16",
   "metadata": {},
   "source": [
    "## 4. Exploration / Exploitation : algorithmes\n",
    "\n",
    "### 4.1 Epsilon‑gourmand (\\\\(\\\\varepsilon\\\\)-greedy)\n",
    "\n",
    "\\\\[\n",
    "a_t =\n",
    "\\\\begin{cases}\n",
    "\\text{argmax}_i \\\\, Q_i, & \\\\text{avec proba } 1-\\\\varepsilon, \\\\\\\\\n",
    "\\text{action aléatoire}, & \\\\text{avec proba } \\\\varepsilon.\n",
    "\\\\end{cases}\n",
    "\\\\]\n",
    "\n",
    "### 4.2 Upper Confidence Bound (UCB‑1)\n",
    "\n",
    "\\\\[\n",
    "a_t = \\\\,\\underset{i}{\\\\operatorname{argmax}}\\\\,\n",
    "\\\\Bigl[\\\\bar X_i + c\\\\,\\\\sqrt{\\\\tfrac{2\\\\ln t}{n_i}}\\\\Bigr]\n",
    "\\\\]\n",
    "\n",
    "où  \n",
    "* \\\\(\\\\bar X_i\\\\) : gain moyen observé du bras \\\\(i\\\\),  \n",
    "* \\\\(n_i\\\\) : nombre de tirages du bras \\\\(i\\\\),  \n",
    "* \\\\(t\\\\) : nombre total de tirages,  \n",
    "* \\\\(c\\\\) : paramètre d’optimisme.\n",
    "\n",
    "### 4.3 Thompson Sampling\n",
    "\n",
    "Choisir \\\\(a\\\\sim \\\\text{argmax}_{i}\\\\, \\\\tilde\\\\theta_i\\\\) où \\\\(\\\\tilde\\\\theta_i\\\\) est un échantillon de la postérieure \\\\(P(\\\\theta_i \\\\mid \\\\text{données})\\\\).\n",
    "\n",
    "#### Illustration : bandit 10 bras, \\\\(\\\\varepsilon=0.1\\\\)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8295a205",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(0)\n",
    "n_arms = 10\n",
    "true_p = np.random.beta(2,5, size=n_arms)   # vraies proba de gain\n",
    "Q = np.zeros(n_arms)\n",
    "N = np.zeros(n_arms)\n",
    "eps = 0.1\n",
    "T = 2000\n",
    "rewards = []\n",
    "\n",
    "for t in range(T):\n",
    "    if np.random.rand() < eps:\n",
    "        a = np.random.randint(n_arms)\n",
    "    else:\n",
    "        a = np.argmax(Q)\n",
    "    r = 1 if np.random.rand() < true_p[a] else 0\n",
    "    rewards.append(r)\n",
    "    N[a] += 1\n",
    "    Q[a] += (r - Q[a]) / N[a]          # MAJ incrémentale\n",
    "\n",
    "cumrew = np.cumsum(rewards)\n",
    "plt.plot(cumrew / (np.arange(T)+1))\n",
    "plt.xlabel('Essai')\n",
    "plt.ylabel('Gain moyen cumulatif')\n",
    "plt.title('Bandit ε‑greedy (ε=0.1)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6f9438",
   "metadata": {},
   "source": [
    "**Exercice 4.1 :**  \n",
    "*Implémente le même bandit avec l’algorithme **UCB‑1** et compare le gain moyen obtenu.*  \n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d361b0d",
   "metadata": {},
   "source": [
    "## 5. Analogies physiques\n",
    "\n",
    "| Concept cog./IA | Analogue physique | Équation clé |\n",
    "|-----------------|-------------------|--------------|\n",
    "| Descente de gradient | Mouvement d’une particule dans un potentiel visqueux | \\\\(\\\\theta_{t+1}=\\\\theta_t-\\\\eta\\,\\\\nabla J(\\\\theta_t)\\\\) |\n",
    "| Drift‑diffusion (décision) | Mouvement brownien avec dérive | \\\\(dx = k\\,dt + \\\\sigma\\,dW_t\\\\) |\n",
    "| Free‑energy (Friston) | Potentiel Helmholtz \\\\(F=E-TS\\\\) en thermo‑stat | \\\\(\\\\mathcal F = \\\\langle \\\\ln q - \\\\ln p \\\\rangle_q\\\\) |\n",
    "| UCB | Inégalité de concentration de Hoeffding | \\\\(\\\\bar X_i + c\\\\sqrt{\\\\frac{2\\\\ln t}{n_i}}\\\\) |\n",
    "| Thompson Sampling | Mécanique statistique : échantillonnage Boltzmann | \\\\(p(\\\\theta\\\\mid D)\\\\propto p(D\\\\mid\\\\theta)p(\\\\theta)\\\\) |\n",
    "\n",
    "### 5.1 Principe de moindre action (rappel)\n",
    "\n",
    "En mécanique analytique, **l’action** :\n",
    "\n",
    "\\\\[\n",
    "S=\\\\int_{t_0}^{t_1} L(q,\\\\dot q,t)\\\\,dt\n",
    "\\\\]\n",
    "\n",
    "La trajectoire réelle minimise \\\\(S\\\\).  \n",
    "Analogie : un algorithme cherche un minimum de **fonction coût**.\n",
    "\n",
    "### 5.2 Simulation : descente de gradient sur une fonction à deux minima\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03be02df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def J(theta):\n",
    "    return (theta**4)/4 - (theta**2)/2 + 0.1*theta\n",
    "\n",
    "def dJ(theta):\n",
    "    return theta**3 - theta + 0.1\n",
    "\n",
    "theta = 2.5\n",
    "eta = 0.01\n",
    "traj = []\n",
    "for _ in range(5000):\n",
    "    traj.append(theta)\n",
    "    theta -= eta * dJ(theta)\n",
    "\n",
    "theta_grid = np.linspace(-2.5, 2.5, 400)\n",
    "plt.plot(theta_grid, J(theta_grid), label='J(θ)')\n",
    "plt.plot(traj, J(np.array(traj)), '.', alpha=0.3, label='Descente')\n",
    "plt.xlabel('θ')\n",
    "plt.ylabel('J(θ)')\n",
    "plt.legend()\n",
    "plt.title('Descente de gradient (analog. physique)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a620bcd",
   "metadata": {},
   "source": [
    "**Exercice 5.1 :**  \n",
    "*Modifie \\\\(\\\\eta\\\\) (taux d’apprentissage) ou la dérivée \\\\(dJ\\\\) pour voir comment cela affecte la convergence. Compare avec le concept de température dans l’algorithme softmax.*  \n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e1825b",
   "metadata": {},
   "source": [
    "## Solutions (cliquer pour dérouler)\n",
    "\n",
    "*(Les cellules suivantes contiennent des solutions possibles aux exercices. Masque‑les si tu souhaites d’abord essayer par toi‑même.)*  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
