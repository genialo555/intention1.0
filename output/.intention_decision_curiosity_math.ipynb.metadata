{"timestamp": 1748515796.525663, "stored_source_code": "# declare a list tasks whose products you want to use as inputs\nupstream = None\n\n# Intention, D\u00e9cision & Curiosit\u00e9 \u2013 Formules Math\u00e9matiques & Physiques\n\nCe carnet Jupyter rassemble **les principales \u00e9quations** issues de la psychologie quantitative, de l\u2019\u00e9conomie comportementale, de la neuroscience computationnelle **et des analogies physiques** qui sous\u2011tendent les concepts\u00a0d\u2019intention, de prise de d\u00e9cision et de curiosit\u00e9 intrins\u00e8que.  \nChaque section contient\u00a0:\n\n* un rappel th\u00e9orique avec formules en \\\\( \\LaTeX \\\\),\n* des cellules Python illustratives (simulations, trac\u00e9s),\n* des **exercices**\u00a0(+ solutions en fin de section).\n\nEx\u00e9cute les cellules pas \u00e0 pas, modifie les param\u00e8tres et\u2026 reste curieux\u00a0!\n\n## 1. Mod\u00e9liser l\u2019intention\n\n### 1.1 Th\u00e9orie du comportement planifi\u00e9 (Ajzen)\n\n\\\\[\n\\text{Intention} = w_1 \\, \\text{Attitude} \\;+\\; w_2 \\, \\text{Norme\u00a0subjective} \\;+\\; w_3 \\, \\text{Contr\u00f4le\u00a0per\u00e7u}\n\\\\]\n\no\u00f9  \n* **Attitude**\u00a0: \u00e9valuation personnelle du comportement,  \n* **Norme subjective**\u00a0: pression sociale per\u00e7ue,  \n* **Contr\u00f4le comportemental per\u00e7u**\u00a0: facilit\u00e9/difficult\u00e9 anticip\u00e9e,  \n* \\\\(w_i\\\\)\u00a0: poids (estim\u00e9s par r\u00e9gression logistique).\n\n### 1.2 Impl\u00e9mentation\u00a0: pr\u00e9dire l\u2019intention de pratiquer une activit\u00e9 physique\n\nDans l\u2019exemple ci\u2011dessous, on\u00a0:\n\n1. g\u00e9n\u00e8re un \u00e9chantillon synth\u00e9tique (N\u00a0=\u00a0250),\n2. ajuste une **r\u00e9gression logistique**,\n3. visualise la probabilit\u00e9 d\u2019intention en fonction des trois pr\u00e9dicteurs.\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\n\n# G\u00e9n\u00e9ration de donn\u00e9es synth\u00e9tiques\nnp.random.seed(42)\nN = 250\natt = np.random.normal(0, 1, N)\nnorm = np.random.normal(0, 1, N)\nctrl = np.random.normal(0, 1, N)\n# vrais poids\nw = np.array([1.2, 0.8, 1.0])\nz = 0.5 + w[0]*att + w[1]*norm + w[2]*ctrl\nprob = 1/(1+np.exp(-z))\nintent = (np.random.rand(N) < prob).astype(int)\n\ndf = pd.DataFrame({'Attitude': att, 'Norme': norm, 'Contr\u00f4le': ctrl, 'Intention': intent})\n\n# Mod\u00e8le\nX = df[['Attitude', 'Norme', 'Contr\u00f4le']]\ny = df['Intention']\nmodel = LogisticRegression()\nmodel.fit(X, y)\n\nprint('Poids estim\u00e9s\u00a0:', np.round(model.coef_[0], 2))\nprint('Intercept\u00a0:   ', np.round(model.intercept_, 2))\n\n# Visualisation 2\u2011D pour Attitude vs Probabilit\u00e9 (les autres fix\u00e9s \u00e0 0)\natt_grid = np.linspace(-3, 3, 200)\nX_plot = np.c_[att_grid, np.zeros_like(att_grid), np.zeros_like(att_grid)]\np_plot = model.predict_proba(X_plot)[:,1]\n\nplt.plot(att_grid, p_plot)\nplt.xlabel('Attitude (\u00e9cart\u2011type)')\nplt.ylabel('P(Intention\u202f=\u202f1)')\nplt.title(\"Probabilit\u00e9 d'intention selon l'attitude\")\nplt.grid(True)\nplt.show()\n**Exercice\u00a01.1\u00a0:**  \n*Change les poids \\\\(w_i\\\\) (ou le nombre d\u2019observations) et observe comment la courbe logistique \u00e9volue.*  \n\n---\n\n\n## 2. Th\u00e9orie de la d\u00e9cision\n\n### 2.1 Utilit\u00e9 esp\u00e9r\u00e9e\n\n\\\\[\nEU(a) = \\sum_{i=1}^n p_i \\, u(x_i)\n\\\\]\n\no\u00f9 \\\\(p_i\\\\) est la probabilit\u00e9 de l\u2019issue \\\\(x_i\\\\), et \\\\(u\\\\) la fonction d\u2019utilit\u00e9.\n\n### 2.2 Th\u00e9orie des perspectives (Kahneman\u00a0&\u00a0Tversky)\n\nValeur\u00a0:\n\n\\\\[\nv(x)=\n\\\\begin{cases}\n  x^{\\\\alpha}, & \\\\text{si } x \\\\ge 0,\\\\\\\\\n  -\\\\lambda\\\\,(-x)^{\\\\beta}, & \\\\text{si } x < 0,\n\\\\end{cases}\n\\\\]\n\navec \\\\(\\\\lambda>1\\\\) (aversion aux pertes), \\\\(0<\\\\alpha,\\\\beta<1\\\\).\n\nPond\u00e9ration des probabilit\u00e9s\u00a0:\n\n\\\\[\n\\\\pi(p)=\\\\frac{p^{\\\\gamma}}{\\\\bigl(p^{\\\\gamma}+(1-p)^{\\\\gamma}\\\\bigr)^{1/\\\\gamma}}\n\\\\]\n\n### 2.3 R\u00e8gle softmax\n\n\\\\[\nP(a)=\\\\frac{\\\\exp(\\\\beta Q(a))}{\\\\sum_b \\\\exp(\\\\beta Q(b))}\n\\\\]\n\navec \\\\(\\\\beta\\\\)\u00a0: *\u201cinverse temperature\u201d* (contr\u00f4le l\u2019exploitation/exploration).\n\n### 2.4 Simulation du mod\u00e8le drift\u2011diffusion (DDM)\n\nLe DDM mod\u00e9lise le temps et l\u2019exactitude d\u2019une d\u00e9cision binaire\u00a0:\n\n\\\\[\ndx_t = k \\\\, dt + \\\\sigma \\\\, dW_t,\n\\\\quad\n\\\\text{d\u00e9cision quand } |x_t| \\\\ge A\n\\\\]\n\no\u00f9  \n* \\\\(k\\\\)\u00a0: d\u00e9rive (evidence),  \n* \\\\(\\\\sigma\\\\)\u00a0: bruit,  \n* \\\\(A\\\\)\u00a0: seuil.\n\nLe code suivant simule 500\u00a0trials et trace la distribution des temps de d\u00e9cision. \n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef simulate_ddm(n_trials=500, k=0.3, sigma=1.0, A=1.0, dt=0.01):\n    rts = []\n    choices = []\n    for _ in range(n_trials):\n        x = 0\n        t = 0\n        while abs(x) < A:\n            dx = k*dt + sigma*np.sqrt(dt)*np.random.randn()\n            x += dx\n            t += dt\n        rts.append(t)\n        choices.append(int(x > 0))\n    return np.array(rts), np.array(choices)\n\nrts, choices = simulate_ddm()\nprint('Exactitude\u00a0:', choices.mean())\nplt.hist(rts, bins=30)\nplt.xlabel('Temps de d\u00e9cision (s)')\nplt.ylabel('Fr\u00e9quence')\nplt.title('Distribution des RT (DDM simul\u00e9)')\nplt.show()\n**Exercice\u00a02.1\u00a0:**  \n*Varie \\\\(k\\\\), \\\\(\\\\sigma\\\\) ou \\\\(A\\\\) et mesure l\u2019effet sur la vitesse et la pr\u00e9cision.*  \n\n---\n\n\n## 3. Curiosit\u00e9 & th\u00e9orie de l\u2019information\n\n### 3.1 Entropie de Shannon  \n\n\\\\[\nH(X) = -\\\\sum_{x} p(x)\\\\,\\\\log_2 p(x)\n\\\\]\n\n### 3.2 Divergence de Kullback\u2013Leibler  \n\n\\\\[\nD_{\\\\text{KL}}\\\\bigl(P \\\\parallel Q\\\\bigr)=\\\\sum_x P(x)\\\\,\\\\log\\\\frac{P(x)}{Q(x)}\n\\\\]\n\n### 3.3 Gain d\u2019information (mutual information)\n\n\\\\[\nI(X;Y)=H(X)-H(X\\\\mid Y)\n\\\\]\n\n### 3.4 Free\u2011energy (active inference, Friston)\n\n\\\\[\n\\\\mathcal F = \\\\mathbb E_q\\\\bigl[\\\\ln q(s) - \\\\ln p(s,o)\\\\bigr]\n\\\\]\n\nMinimiser \\\\(\\\\mathcal F\\\\) \u2248 maximiser l\u2019**exactitude** tout en minimisant la **complexit\u00e9**.\n\n### 3.5 Bonus de curiosit\u00e9 (ICM)\n\n\\\\[\nR_t^{\\\\text{int}} = \\\\tfrac{1}{2}\\\\Bigl\\\\| \\\\hat\\\\phi(s_{t+1}) - \\\\phi(s_{t+1}) \\\\Bigr\\\\|^2\n\\\\]\n\navec \\\\(\\\\phi\\\\)\u00a0: encodeur d\u2019\u00e9tat, \\\\(\\\\hat\\\\phi\\\\)\u00a0: pr\u00e9diction du mod\u00e8le dynamique interne.\n\n#### Exemple\u00a0: entropie d\u2019une pi\u00e8ce biais\u00e9e\n\nimport numpy as np\nimport math\n\np = 0.3  # probabilit\u00e9 de pile\nentropy = - (p*math.log2(p) + (1-p)*math.log2(1-p))\nprint(f\"Entropie d'une pi\u00e8ce biais\u00e9e (p={p}) : {entropy:.3f} bits\")\n**Exercice\u00a03.1\u00a0:**  \n*\u00c9cris une fonction qui prend une liste de probabilit\u00e9s \\\\(p_i\\\\) et renvoie l\u2019entropie correspondante. Teste\u2011la avec une loi uniforme (maximum d\u2019entropie) puis avec une loi tr\u00e8s concentr\u00e9e.*  \n\n---\n\n\n## 4. Exploration\u202f/\u202fExploitation\u00a0: algorithmes\n\n### 4.1 Epsilon\u2011gourmand (\\\\(\\\\varepsilon\\\\)-greedy)\n\n\\\\[\na_t =\n\\\\begin{cases}\n\\text{argmax}_i \\\\, Q_i, & \\\\text{avec proba } 1-\\\\varepsilon, \\\\\\\\\n\\text{action al\u00e9atoire}, & \\\\text{avec proba } \\\\varepsilon.\n\\\\end{cases}\n\\\\]\n\n### 4.2 Upper Confidence Bound (UCB\u20111)\n\n\\\\[\na_t = \\\\,\\underset{i}{\\\\operatorname{argmax}}\\\\,\n\\\\Bigl[\\\\bar X_i + c\\\\,\\\\sqrt{\\\\tfrac{2\\\\ln t}{n_i}}\\\\Bigr]\n\\\\]\n\no\u00f9  \n* \\\\(\\\\bar X_i\\\\)\u00a0: gain moyen observ\u00e9 du bras\u00a0\\\\(i\\\\),  \n* \\\\(n_i\\\\)\u00a0: nombre de tirages du bras\u00a0\\\\(i\\\\),  \n* \\\\(t\\\\)\u00a0: nombre total de tirages,  \n* \\\\(c\\\\)\u00a0: param\u00e8tre d\u2019optimisme.\n\n### 4.3 Thompson\u00a0Sampling\n\nChoisir \\\\(a\\\\sim \\\\text{argmax}_{i}\\\\, \\\\tilde\\\\theta_i\\\\) o\u00f9 \\\\(\\\\tilde\\\\theta_i\\\\) est un \u00e9chantillon de la post\u00e9rieure \\\\(P(\\\\theta_i \\\\mid \\\\text{donn\u00e9es})\\\\).\n\n#### Illustration\u00a0: bandit 10\u00a0bras, \\\\(\\\\varepsilon=0.1\\\\)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(0)\nn_arms = 10\ntrue_p = np.random.beta(2,5, size=n_arms)   # vraies proba de gain\nQ = np.zeros(n_arms)\nN = np.zeros(n_arms)\neps = 0.1\nT = 2000\nrewards = []\n\nfor t in range(T):\n    if np.random.rand() < eps:\n        a = np.random.randint(n_arms)\n    else:\n        a = np.argmax(Q)\n    r = 1 if np.random.rand() < true_p[a] else 0\n    rewards.append(r)\n    N[a] += 1\n    Q[a] += (r - Q[a]) / N[a]          # MAJ incr\u00e9mentale\n\ncumrew = np.cumsum(rewards)\nplt.plot(cumrew / (np.arange(T)+1))\nplt.xlabel('Essai')\nplt.ylabel('Gain moyen cumulatif')\nplt.title('Bandit \u03b5\u2011greedy (\u03b5=0.1)')\nplt.show()\n**Exercice\u00a04.1\u00a0:**  \n*Impl\u00e9mente le m\u00eame bandit avec l\u2019algorithme **UCB\u20111** et compare le gain moyen obtenu.*  \n\n---\n\n\n## 5. Analogies physiques\n\n| Concept cog./IA | Analogue physique | \u00c9quation cl\u00e9 |\n|-----------------|-------------------|--------------|\n| Descente de gradient | Mouvement d\u2019une particule dans un potentiel visqueux | \\\\(\\\\theta_{t+1}=\\\\theta_t-\\\\eta\\,\\\\nabla J(\\\\theta_t)\\\\) |\n| Drift\u2011diffusion (d\u00e9cision) | Mouvement brownien avec d\u00e9rive | \\\\(dx = k\\,dt + \\\\sigma\\,dW_t\\\\) |\n| Free\u2011energy (Friston) | Potentiel Helmholtz \\\\(F=E-TS\\\\) en thermo\u2011stat | \\\\(\\\\mathcal F = \\\\langle \\\\ln q - \\\\ln p \\\\rangle_q\\\\) |\n| UCB | In\u00e9galit\u00e9 de concentration de Hoeffding | \\\\(\\\\bar X_i + c\\\\sqrt{\\\\frac{2\\\\ln t}{n_i}}\\\\) |\n| Thompson\u00a0Sampling | M\u00e9canique statistique\u00a0: \u00e9chantillonnage Boltzmann | \\\\(p(\\\\theta\\\\mid D)\\\\propto p(D\\\\mid\\\\theta)p(\\\\theta)\\\\) |\n\n### 5.1 Principe de moindre action (rappel)\n\nEn m\u00e9canique analytique, **l\u2019action**\u00a0:\n\n\\\\[\nS=\\\\int_{t_0}^{t_1} L(q,\\\\dot q,t)\\\\,dt\n\\\\]\n\nLa trajectoire r\u00e9elle minimise \\\\(S\\\\).  \nAnalogie\u00a0: un algorithme cherche un minimum de **fonction co\u00fbt**.\n\n### 5.2 Simulation\u00a0: descente de gradient sur une fonction \u00e0 deux minima\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef J(theta):\n    return (theta**4)/4 - (theta**2)/2 + 0.1*theta\n\ndef dJ(theta):\n    return theta**3 - theta + 0.1\n\ntheta = 2.5\neta = 0.01\ntraj = []\nfor _ in range(5000):\n    traj.append(theta)\n    theta -= eta * dJ(theta)\n\ntheta_grid = np.linspace(-2.5, 2.5, 400)\nplt.plot(theta_grid, J(theta_grid), label='J(\u03b8)')\nplt.plot(traj, J(np.array(traj)), '.', alpha=0.3, label='Descente')\nplt.xlabel('\u03b8')\nplt.ylabel('J(\u03b8)')\nplt.legend()\nplt.title('Descente de gradient (analog. physique)')\nplt.show()\n**Exercice\u00a05.1\u00a0:**  \n*Modifie \\\\(\\\\eta\\\\) (taux d\u2019apprentissage) ou la d\u00e9riv\u00e9e \\\\(dJ\\\\) pour voir comment cela affecte la convergence. Compare avec le concept de temp\u00e9rature dans l\u2019algorithme softmax.*  \n\n---\n\n\n## Solutions (cliquer pour d\u00e9rouler)\n\n*(Les cellules suivantes contiennent des solutions possibles aux exercices. Masque\u2011les si tu souhaites d\u2019abord essayer par toi\u2011m\u00eame.)*  \n", "params": {}}